{"cells":[{"cell_type":"markdown","metadata":{},"source":["NOTE: To run this, download this package from Kaggle...\n","\n","https://www.kaggle.com/robikscube/deepfakemodelspackages\n","\n","And place into a folder titled 'input' within your working directory."]},{"cell_type":"markdown","metadata":{},"source":["# Running FaceForensics++ in a Kaggle Notebook\n","\n","![](https://github.com/ondyari/FaceForensics/raw/master/images/teaser.png)\n","\n","In this notebook I test out the state of the art FaceForensics++ package on the provided dataset. Later in this notebook I modify the code slightly to predict for our dataset.\n","\n","This notebook imports from a dataset for:\n","- dlib package wheel (needs to be installed for code to work)\n","- other required packages not in the default kernel\n","- pretrained models by FaceForensics++\n","\n","- The paper that was published can be found here: https://arxiv.org/pdf/1901.08971.pdf\n","- The github repo is here: https://github.com/ondyari/FaceForensics\n","\n","Reference:\n","```\n","@inproceedings{roessler2019faceforensicspp,\n","\tauthor = {Andreas R\\\"ossler and Davide Cozzolino and Luisa Verdoliva and Christian Riess and Justus Thies and Matthias Nie{\\ss}ner},\n","\ttitle = {Face{F}orensics++: Learning to Detect Manipulated Facial Images},\n","\tbooktitle= {International Conference on Computer Vision (ICCV)},\n","\tyear = {2019}\n","}\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pylab as plt\n","import cv2\n","import seaborn as sns\n","from sklearn.metrics import log_loss\n","XCEPTION_MODEL = '../input/deepfakemodelspackages/xception-b5690688.pth'"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["%%time\n","# Install packages\n","!pip install ../input/deepfakemodelspackages/Pillow-6.2.1-cp36-cp36m-manylinux1_x86_64.whl -f ./ --no-index\n","!pip install ../input/deepfakemodelspackages/munch-2.5.0-py2.py3-none-any.whl -f ./ --no-index\n","!pip install ../input/deepfakemodelspackages/numpy-1.17.4-cp36-cp36m-manylinux1_x86_64.whl -f ./ --no-index\n","!pip install ../input/deepfakemodelspackages/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4/ -f ./ --no-index\n","!pip install ../input/deepfakemodelspackages/six-1.13.0-py2.py3-none-any.whl -f ./ --no-index\n","!pip install ../input/deepfakemodelspackages/torchvision-0.4.2-cp36-cp36m-manylinux1_x86_64.whl -f ./ --no-index\n","!pip install ../input/deepfakemodelspackages/tqdm-4.40.2-py2.py3-none-any.whl -f ./ --no-index"]},{"cell_type":"markdown","metadata":{},"source":["## Install dlib\n","Building takes roughly ~6 minutes"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["%%time\n","!pip install ../input/deepfakemodelspackages/dlib-19.19.0/dlib-19.19.0/ -f ./ --no-index"]},{"cell_type":"markdown","metadata":{},"source":["# Copy in FaceForensics Code and Modify to run in kernel\n","\n","- Change the `models.py` file to reference our `xception-b5690688.pth` file.\n","- Change the code to be provided the pytorch model and not the model path. This works better for our purposes.\n","Change the line here:\n","```\n","        state_dict = torch.load(\n","            #'/home/ondyari/.torch/models/xception-b5690688.pth')\n","            XCEPTION_MODEL)\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["## xception.py\n","\"\"\"\n","Ported to pytorch thanks to [tstandley](https://github.com/tstandley/Xception-PyTorch)\n","@author: tstandley\n","Adapted by cadene\n","Creates an Xception Model as defined in:\n","Francois Chollet\n","Xception: Deep Learning with Depthwise Separable Convolutions\n","https://arxiv.org/pdf/1610.02357.pdf\n","This weights ported from the Keras implementation. Achieves the following performance on the validation set:\n","Loss:0.9173 Prec@1:78.892 Prec@5:94.292\n","REMEMBER to set your image size to 3x299x299 for both test and validation\n","normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","                                  std=[0.5, 0.5, 0.5])\n","The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n","\"\"\"\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.model_zoo as model_zoo\n","from torch.nn import init\n","\n","pretrained_settings = {\n","    'xception': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 299, 299],\n","            'input_range': [0, 1],\n","            'mean': [0.5, 0.5, 0.5],\n","            'std': [0.5, 0.5, 0.5],\n","            'num_classes': 1000,\n","            'scale': 0.8975 # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n","        }\n","    }\n","}\n","\n","\n","class SeparableConv2d(nn.Module):\n","    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n","        super(SeparableConv2d,self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n","        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n","\n","    def forward(self,x):\n","        x = self.conv1(x)\n","        x = self.pointwise(x)\n","        return x\n","\n","\n","class Block(nn.Module):\n","    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n","        super(Block, self).__init__()\n","\n","        if out_filters != in_filters or strides!=1:\n","            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n","            self.skipbn = nn.BatchNorm2d(out_filters)\n","        else:\n","            self.skip=None\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        rep=[]\n","\n","        filters=in_filters\n","        if grow_first:\n","            rep.append(self.relu)\n","            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n","            rep.append(nn.BatchNorm2d(out_filters))\n","            filters = out_filters\n","\n","        for i in range(reps-1):\n","            rep.append(self.relu)\n","            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n","            rep.append(nn.BatchNorm2d(filters))\n","\n","        if not grow_first:\n","            rep.append(self.relu)\n","            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n","            rep.append(nn.BatchNorm2d(out_filters))\n","\n","        if not start_with_relu:\n","            rep = rep[1:]\n","        else:\n","            rep[0] = nn.ReLU(inplace=False)\n","\n","        if strides != 1:\n","            rep.append(nn.MaxPool2d(3,strides,1))\n","        self.rep = nn.Sequential(*rep)\n","\n","    def forward(self,inp):\n","        x = self.rep(inp)\n","\n","        if self.skip is not None:\n","            skip = self.skip(inp)\n","            skip = self.skipbn(skip)\n","        else:\n","            skip = inp\n","\n","        x+=skip\n","        return x\n","\n","\n","class Xception(nn.Module):\n","    \"\"\"\n","    Xception optimized for the ImageNet dataset, as specified in\n","    https://arxiv.org/pdf/1610.02357.pdf\n","    \"\"\"\n","    def __init__(self, num_classes=1000):\n","        \"\"\" Constructor\n","        Args:\n","            num_classes: number of classes\n","        \"\"\"\n","        super(Xception, self).__init__()\n","        self.num_classes = num_classes\n","\n","        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        #do relu here\n","\n","        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n","        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n","        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n","\n","        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","\n","        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","\n","        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n","\n","        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n","        self.bn3 = nn.BatchNorm2d(1536)\n","\n","        #do relu here\n","        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n","        self.bn4 = nn.BatchNorm2d(2048)\n","\n","        self.fc = nn.Linear(2048, num_classes)\n","\n","        # #------- init weights --------\n","        # for m in self.modules():\n","        #     if isinstance(m, nn.Conv2d):\n","        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n","        #     elif isinstance(m, nn.BatchNorm2d):\n","        #         m.weight.data.fill_(1)\n","        #         m.bias.data.zero_()\n","        # #-----------------------------\n","\n","    def features(self, input):\n","        x = self.conv1(input)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","\n","        x = self.block1(x)\n","        x = self.block2(x)\n","        x = self.block3(x)\n","        x = self.block4(x)\n","        x = self.block5(x)\n","        x = self.block6(x)\n","        x = self.block7(x)\n","        x = self.block8(x)\n","        x = self.block9(x)\n","        x = self.block10(x)\n","        x = self.block11(x)\n","        x = self.block12(x)\n","\n","        x = self.conv3(x)\n","        x = self.bn3(x)\n","        x = self.relu(x)\n","\n","        x = self.conv4(x)\n","        x = self.bn4(x)\n","        return x\n","\n","    def logits(self, features):\n","        x = self.relu(features)\n","\n","        x = F.adaptive_avg_pool2d(x, (1, 1))\n","        x = x.view(x.size(0), -1)\n","        x = self.last_linear(x)\n","        return x\n","\n","    def forward(self, input):\n","        x = self.features(input)\n","        x = self.logits(x)\n","        return x\n","\n","\n","def xception(num_classes=1000, pretrained='imagenet'):\n","    model = Xception(num_classes=num_classes)\n","    if pretrained:\n","        settings = pretrained_settings['xception'][pretrained]\n","        assert num_classes == settings['num_classes'], \\\n","            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n","\n","        model = Xception(num_classes=num_classes)\n","        model.load_state_dict(model_zoo.load_url(settings['url']))\n","\n","        model.input_space = settings['input_space']\n","        model.input_size = settings['input_size']\n","        model.input_range = settings['input_range']\n","        model.mean = settings['mean']\n","        model.std = settings['std']\n","\n","    # TODO: ugly\n","    model.last_linear = model.fc\n","    del model.fc\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["## models.py\n","\"\"\"\n","Author: Andreas Rössler\n","\"\"\"\n","import os\n","import argparse\n","\n","\n","import torch\n","# import pretrainedmodels\n","import torch.nn as nn\n","import torch.nn.functional as F\n","# from network.xception import xception\n","import math\n","import torchvision\n","\n","\n","def return_pytorch04_xception(pretrained=True):\n","    # Raises warning \"src not broadcastable to dst\" but thats fine\n","    model = xception(pretrained=False)\n","    if pretrained:\n","        # Load model in torch 0.4+\n","        model.fc = model.last_linear\n","        del model.last_linear\n","        state_dict = torch.load(\n","            #'/home/ondyari/.torch/models/xception-b5690688.pth')\n","            XCEPTION_MODEL)\n","        for name, weights in state_dict.items():\n","            if 'pointwise' in name:\n","                state_dict[name] = weights.unsqueeze(-1).unsqueeze(-1)\n","        model.load_state_dict(state_dict)\n","        model.last_linear = model.fc\n","        del model.fc\n","    return model\n","\n","\n","class TransferModel(nn.Module):\n","    \"\"\"\n","    Simple transfer learning model that takes an imagenet pretrained model with\n","    a fc layer as base model and retrains a new fc layer for num_out_classes\n","    \"\"\"\n","    def __init__(self, modelchoice, num_out_classes=2, dropout=0.0):\n","        super(TransferModel, self).__init__()\n","        self.modelchoice = modelchoice\n","        if modelchoice == 'xception':\n","            self.model = return_pytorch04_xception()\n","            # Replace fc\n","            num_ftrs = self.model.last_linear.in_features\n","            if not dropout:\n","                self.model.last_linear = nn.Linear(num_ftrs, num_out_classes)\n","            else:\n","                print('Using dropout', dropout)\n","                self.model.last_linear = nn.Sequential(\n","                    nn.Dropout(p=dropout),\n","                    nn.Linear(num_ftrs, num_out_classes)\n","                )\n","        elif modelchoice == 'resnet50' or modelchoice == 'resnet18':\n","            if modelchoice == 'resnet50':\n","                self.model = torchvision.models.resnet50(pretrained=True)\n","            if modelchoice == 'resnet18':\n","                self.model = torchvision.models.resnet18(pretrained=True)\n","            # Replace fc\n","            num_ftrs = self.model.fc.in_features\n","            if not dropout:\n","                self.model.fc = nn.Linear(num_ftrs, num_out_classes)\n","            else:\n","                self.model.fc = nn.Sequential(\n","                    nn.Dropout(p=dropout),\n","                    nn.Linear(num_ftrs, num_out_classes)\n","                )\n","        else:\n","            raise Exception('Choose valid model, e.g. resnet50')\n","\n","    def set_trainable_up_to(self, boolean, layername=\"Conv2d_4a_3x3\"):\n","        \"\"\"\n","        Freezes all layers below a specific layer and sets the following layers\n","        to true if boolean else only the fully connected final layer\n","        :param boolean:\n","        :param layername: depends on network, for inception e.g. Conv2d_4a_3x3\n","        :return:\n","        \"\"\"\n","        # Stage-1: freeze all the layers\n","        if layername is None:\n","            for i, param in self.model.named_parameters():\n","                param.requires_grad = True\n","                return\n","        else:\n","            for i, param in self.model.named_parameters():\n","                param.requires_grad = False\n","        if boolean:\n","            # Make all layers following the layername layer trainable\n","            ct = []\n","            found = False\n","            for name, child in self.model.named_children():\n","                if layername in ct:\n","                    found = True\n","                    for params in child.parameters():\n","                        params.requires_grad = True\n","                ct.append(name)\n","            if not found:\n","                raise Exception('Layer not found, cant finetune!'.format(\n","                    layername))\n","        else:\n","            if self.modelchoice == 'xception':\n","                # Make fc trainable\n","                for param in self.model.last_linear.parameters():\n","                    param.requires_grad = True\n","\n","            else:\n","                # Make fc trainable\n","                for param in self.model.fc.parameters():\n","                    param.requires_grad = True\n","\n","    def forward(self, x):\n","        x = self.model(x)\n","        return x\n","\n","\n","def model_selection(modelname, num_out_classes,\n","                    dropout=None):\n","    \"\"\"\n","    :param modelname:\n","    :return: model, image size, pretraining<yes/no>, input_list\n","    \"\"\"\n","    if modelname == 'xception':\n","        return TransferModel(modelchoice='xception',\n","                             num_out_classes=num_out_classes), 299, \\\n","               True, ['image'], None\n","    elif modelname == 'resnet18':\n","        return TransferModel(modelchoice='resnet18', dropout=dropout,\n","                             num_out_classes=num_out_classes), \\\n","               224, True, ['image'], None\n","    else:\n","        raise NotImplementedError(modelname)\n","\n","# if __name__ == '__main__':\n","#     model, image_size, *_ = model_selection('resnet18', num_out_classes=2)\n","#     print(model)\n","#     model = model.cuda()\n","#     from torchsummary import summary\n","#     input_s = (3, image_size, image_size)\n","#     print(summary(model, input_s))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["## transform.py\n","\"\"\"\n","Author: Andreas Rössler\n","\"\"\"\n","from torchvision import transforms\n","\n","xception_default_data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize((299, 299)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.5]*3, [0.5]*3)\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize((299, 299)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.5] * 3, [0.5] * 3)\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.Resize((299, 299)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.5] * 3, [0.5] * 3)\n","    ]),\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["## detect_from_video.py\n","\"\"\"\n","Evaluates a folder of video files or a single file with a xception binary\n","classification network.\n","Usage:\n","python detect_from_video.py\n","    -i <folder with video files or path to video file>\n","    -m <path to model file>\n","    -o <path to output folder, will write one or multiple output videos there>\n","Author: Andreas Rössler\n","\"\"\"\n","import os\n","import argparse\n","from os.path import join\n","import cv2\n","import dlib\n","import torch\n","import torch.nn as nn\n","from PIL import Image as pil_image\n","from tqdm.notebook import tqdm\n","\n","# from network.models import model_selection\n","# from dataset.transform import xception_default_data_transforms\n","\n","def get_boundingbox(face, width, height, scale=1.3, minsize=None):\n","    \"\"\"\n","    Expects a dlib face to generate a quadratic bounding box.\n","    :param face: dlib face class\n","    :param width: frame width\n","    :param height: frame height\n","    :param scale: bounding box size multiplier to get a bigger face region\n","    :param minsize: set minimum bounding box size\n","    :return: x, y, bounding_box_size in opencv form\n","    \"\"\"\n","    x1 = face.left()\n","    y1 = face.top()\n","    x2 = face.right()\n","    y2 = face.bottom()\n","    size_bb = int(max(x2 - x1, y2 - y1) * scale)\n","    if minsize:\n","        if size_bb < minsize:\n","            size_bb = minsize\n","    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n","\n","    # Check for out of bounds, x-y top left corner\n","    x1 = max(int(center_x - size_bb // 2), 0)\n","    y1 = max(int(center_y - size_bb // 2), 0)\n","    # Check for too big bb size for given x, y\n","    size_bb = min(width - x1, size_bb)\n","    size_bb = min(height - y1, size_bb)\n","\n","    return x1, y1, size_bb\n","\n","\n","def preprocess_image(image, cuda=True):\n","    \"\"\"\n","    Preprocesses the image such that it can be fed into our network.\n","    During this process we envoke PIL to cast it into a PIL image.\n","    :param image: numpy image in opencv form (i.e., BGR and of shape\n","    :return: pytorch tensor of shape [1, 3, image_size, image_size], not\n","    necessarily casted to cuda\n","    \"\"\"\n","    # Revert from BGR\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    # Preprocess using the preprocessing function used during training and\n","    # casting it to PIL image\n","    preprocess = xception_default_data_transforms['test']\n","    preprocessed_image = preprocess(pil_image.fromarray(image))\n","    # Add first dimension as the network expects a batch\n","    preprocessed_image = preprocessed_image.unsqueeze(0)\n","    if cuda:\n","        preprocessed_image = preprocessed_image.cuda()\n","    return preprocessed_image\n","\n","\n","def predict_with_model(image, model, post_function=nn.Softmax(dim=1),\n","                       cuda=True):\n","    \"\"\"\n","    Predicts the label of an input image. Preprocesses the input image and\n","    casts it to cuda if required\n","    :param image: numpy image\n","    :param model: torch model with linear layer at the end\n","    :param post_function: e.g., softmax\n","    :param cuda: enables cuda, must be the same parameter as the model\n","    :return: prediction (1 = fake, 0 = real)\n","    \"\"\"\n","    # Preprocess\n","    preprocessed_image = preprocess_image(image, cuda)\n","\n","    # Model prediction\n","    output = model(preprocessed_image)\n","    output = post_function(output)\n","\n","    # Cast to desired\n","    _, prediction = torch.max(output, 1)    # argmax\n","    prediction = float(prediction.cpu().numpy())\n","\n","    return int(prediction), output\n","\n","def test_full_image_network(video_path, model, output_path,\n","                            start_frame=0, end_frame=None, cuda=True):\n","    \"\"\"\n","    Reads a video and evaluates a subset of frames with the a detection network\n","    that takes in a full frame. Outputs are only given if a face is present\n","    and the face is highlighted using dlib.\n","    :param video_path: path to video file\n","    :param model_path: path to model file (should expect the full sized image)\n","    :param output_path: path where the output video is stored\n","    :param start_frame: first frame to evaluate\n","    :param end_frame: last frame to evaluate\n","    :param cuda: enable cuda\n","    :return:\n","    \n","    # Modified to take in the model file instead of model\n","    \"\"\"\n","    #print('Starting: {}'.format(video_path))\n","\n","    # Read and write\n","    reader = cv2.VideoCapture(video_path)\n","\n","    video_fn = video_path.split('/')[-1].split('.')[0]+'.avi'\n","    os.makedirs(output_path, exist_ok=True)\n","    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n","    fps = reader.get(cv2.CAP_PROP_FPS)\n","    num_frames = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n","    writer = None\n","\n","    # Face detector\n","    face_detector = dlib.get_frontal_face_detector()\n","\n","    # Load model\n","#     model, *_ = model_selection(modelname='xception', num_out_classes=2)\n","#     if model_path is not None:\n","#         model = torch.load(model_path)\n","#         print('Model found in {}'.format(model_path))\n","#     else:\n","#         print('No model found, initializing random model.')\n","#     if cuda:\n","#         model = model.cuda()\n","\n","    # Text variables\n","    font_face = cv2.FONT_HERSHEY_SIMPLEX\n","    thickness = 2\n","    font_scale = 1\n","\n","    # Frame numbers and length of output video\n","    frame_num = 0\n","    assert start_frame < num_frames - 1\n","    end_frame = end_frame if end_frame else num_frames\n","    pbar = tqdm(total=end_frame-start_frame)\n","\n","    while reader.isOpened():\n","        _, image = reader.read()\n","        if image is None:\n","            break\n","        frame_num += 1\n","\n","        if frame_num < start_frame:\n","            continue\n","        pbar.update(1)\n","\n","        # Image size\n","#         print('getting image size')\n","        height, width = image.shape[:2]\n","\n","        # Init output writer\n","#         print('init output writer')\n","        if writer is None:\n","            writer = cv2.VideoWriter(join(output_path, video_fn), fourcc, fps,\n","                                     (height, width)[::-1])\n","\n","        # 2. Detect with dlib\n","#         print('detect with dlib')\n","        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","        faces = face_detector(gray, 1)\n","        if len(faces):\n","            # For now only take biggest face\n","            face = faces[0]\n","\n","            # --- Prediction ---------------------------------------------------\n","            # Face crop with dlib and bounding box scale enlargement\n","            x, y, size = get_boundingbox(face, width, height)\n","            cropped_face = image[y:y+size, x:x+size]\n","\n","            # Actual prediction using our model\n","            prediction, output = predict_with_model(cropped_face, model,\n","                                                    cuda=cuda)\n","            # ------------------------------------------------------------------\n","\n","            # Text and bb\n","            x = face.left()\n","            y = face.top()\n","            w = face.right() - x\n","            h = face.bottom() - y\n","            label = 'fake' if prediction == 1 else 'real'\n","            color = (0, 255, 0) if prediction == 0 else (0, 0, 255)\n","            output_list = ['{0:.2f}'.format(float(x)) for x in\n","                           output.detach().cpu().numpy()[0]]\n","            cv2.putText(image, str(output_list)+'=>'+label, (x, y+h+30),\n","                        font_face, font_scale,\n","                        color, thickness, 2)\n","            # draw box over face\n","            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n","\n","        if frame_num >= end_frame:\n","            break\n","\n","        # Show\n","#         print('show result')\n","        # cv2.imshow('test', image)\n","#         cv2.waitKey(33)     # About 30 fps\n","        writer.write(image)\n","    pbar.close()\n","    if writer is not None:\n","        writer.release()\n","        #print('Finished! Output saved under {}'.format(output_path))\n","    else:\n","        pass\n","        #print('Input video file was empty')\n","    return"]},{"cell_type":"markdown","metadata":{},"source":["# Use the `classification` package per the github instructions\n","Reference: https://github.com/ondyari/FaceForensics/tree/master/classification#classification\n","\n","Now that we have the code available to us in the kernel, we will use the instructions as provided on the the github page."]},{"cell_type":"markdown","metadata":{},"source":["# Write Files to disk for importing model\n","- Since we are importing pretrained models, the code expects a `network` package with some files from the github repo\n","- We will write these files to disk so the model import will run correctly."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["!mkdir network"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["%%writefile network/__init__.py\n","# init"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["%%writefile network/xception.py\n","\"\"\"\n","Ported to pytorch thanks to [tstandley](https://github.com/tstandley/Xception-PyTorch)\n","@author: tstandley\n","Adapted by cadene\n","Creates an Xception Model as defined in:\n","Francois Chollet\n","Xception: Deep Learning with Depthwise Separable Convolutions\n","https://arxiv.org/pdf/1610.02357.pdf\n","This weights ported from the Keras implementation. Achieves the following performance on the validation set:\n","Loss:0.9173 Prec@1:78.892 Prec@5:94.292\n","REMEMBER to set your image size to 3x299x299 for both test and validation\n","normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","                                  std=[0.5, 0.5, 0.5])\n","The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n","\"\"\"\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.model_zoo as model_zoo\n","from torch.nn import init\n","\n","pretrained_settings = {\n","    'xception': {\n","        'imagenet': {\n","            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/xception-b5690688.pth',\n","            'input_space': 'RGB',\n","            'input_size': [3, 299, 299],\n","            'input_range': [0, 1],\n","            'mean': [0.5, 0.5, 0.5],\n","            'std': [0.5, 0.5, 0.5],\n","            'num_classes': 1000,\n","            'scale': 0.8975 # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n","        }\n","    }\n","}\n","\n","\n","class SeparableConv2d(nn.Module):\n","    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n","        super(SeparableConv2d,self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n","        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n","\n","    def forward(self,x):\n","        x = self.conv1(x)\n","        x = self.pointwise(x)\n","        return x\n","\n","\n","class Block(nn.Module):\n","    def __init__(self,in_filters,out_filters,reps,strides=1,start_with_relu=True,grow_first=True):\n","        super(Block, self).__init__()\n","\n","        if out_filters != in_filters or strides!=1:\n","            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n","            self.skipbn = nn.BatchNorm2d(out_filters)\n","        else:\n","            self.skip=None\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        rep=[]\n","\n","        filters=in_filters\n","        if grow_first:\n","            rep.append(self.relu)\n","            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n","            rep.append(nn.BatchNorm2d(out_filters))\n","            filters = out_filters\n","\n","        for i in range(reps-1):\n","            rep.append(self.relu)\n","            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n","            rep.append(nn.BatchNorm2d(filters))\n","\n","        if not grow_first:\n","            rep.append(self.relu)\n","            rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n","            rep.append(nn.BatchNorm2d(out_filters))\n","\n","        if not start_with_relu:\n","            rep = rep[1:]\n","        else:\n","            rep[0] = nn.ReLU(inplace=False)\n","\n","        if strides != 1:\n","            rep.append(nn.MaxPool2d(3,strides,1))\n","        self.rep = nn.Sequential(*rep)\n","\n","    def forward(self,inp):\n","        x = self.rep(inp)\n","\n","        if self.skip is not None:\n","            skip = self.skip(inp)\n","            skip = self.skipbn(skip)\n","        else:\n","            skip = inp\n","\n","        x+=skip\n","        return x\n","\n","\n","class Xception(nn.Module):\n","    \"\"\"\n","    Xception optimized for the ImageNet dataset, as specified in\n","    https://arxiv.org/pdf/1610.02357.pdf\n","    \"\"\"\n","    def __init__(self, num_classes=1000):\n","        \"\"\" Constructor\n","        Args:\n","            num_classes: number of classes\n","        \"\"\"\n","        super(Xception, self).__init__()\n","        self.num_classes = num_classes\n","\n","        self.conv1 = nn.Conv2d(3, 32, 3,2, 0, bias=False)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.conv2 = nn.Conv2d(32,64,3,bias=False)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        #do relu here\n","\n","        self.block1=Block(64,128,2,2,start_with_relu=False,grow_first=True)\n","        self.block2=Block(128,256,2,2,start_with_relu=True,grow_first=True)\n","        self.block3=Block(256,728,2,2,start_with_relu=True,grow_first=True)\n","\n","        self.block4=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","        self.block5=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","        self.block6=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","        self.block7=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","\n","        self.block8=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","        self.block9=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","        self.block10=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","        self.block11=Block(728,728,3,1,start_with_relu=True,grow_first=True)\n","\n","        self.block12=Block(728,1024,2,2,start_with_relu=True,grow_first=False)\n","\n","        self.conv3 = SeparableConv2d(1024,1536,3,1,1)\n","        self.bn3 = nn.BatchNorm2d(1536)\n","\n","        #do relu here\n","        self.conv4 = SeparableConv2d(1536,2048,3,1,1)\n","        self.bn4 = nn.BatchNorm2d(2048)\n","\n","        self.fc = nn.Linear(2048, num_classes)\n","\n","        # #------- init weights --------\n","        # for m in self.modules():\n","        #     if isinstance(m, nn.Conv2d):\n","        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n","        #     elif isinstance(m, nn.BatchNorm2d):\n","        #         m.weight.data.fill_(1)\n","        #         m.bias.data.zero_()\n","        # #-----------------------------\n","\n","    def features(self, input):\n","        x = self.conv1(input)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","\n","        x = self.block1(x)\n","        x = self.block2(x)\n","        x = self.block3(x)\n","        x = self.block4(x)\n","        x = self.block5(x)\n","        x = self.block6(x)\n","        x = self.block7(x)\n","        x = self.block8(x)\n","        x = self.block9(x)\n","        x = self.block10(x)\n","        x = self.block11(x)\n","        x = self.block12(x)\n","\n","        x = self.conv3(x)\n","        x = self.bn3(x)\n","        x = self.relu(x)\n","\n","        x = self.conv4(x)\n","        x = self.bn4(x)\n","        return x\n","\n","    def logits(self, features):\n","        x = self.relu(features)\n","\n","        x = F.adaptive_avg_pool2d(x, (1, 1))\n","        x = x.view(x.size(0), -1)\n","        x = self.last_linear(x)\n","        return x\n","\n","    def forward(self, input):\n","        x = self.features(input)\n","        x = self.logits(x)\n","        return x\n","\n","\n","def xception(num_classes=1000, pretrained='imagenet'):\n","    model = Xception(num_classes=num_classes)\n","    if pretrained:\n","        settings = pretrained_settings['xception'][pretrained]\n","        assert num_classes == settings['num_classes'], \\\n","            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n","\n","        model = Xception(num_classes=num_classes)\n","        model.load_state_dict(model_zoo.load_url(settings['url']))\n","\n","        model.input_space = settings['input_space']\n","        model.input_size = settings['input_size']\n","        model.input_range = settings['input_range']\n","        model.mean = settings['mean']\n","        model.std = settings['std']\n","\n","    # TODO: ugly\n","    model.last_linear = model.fc\n","    del model.fc\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["%%writefile network/models.py\n","\"\"\"\n","Author: Andreas Rössler\n","\"\"\"\n","import os\n","import argparse\n","\n","\n","import torch\n","#import pretrainedmodels\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from network.xception import xception\n","import math\n","import torchvision\n","\n","\n","def return_pytorch04_xception(pretrained=True):\n","    # Raises warning \"src not broadcastable to dst\" but thats fine\n","    model = xception(pretrained=False)\n","    if pretrained:\n","        # Load model in torch 0.4+\n","        model.fc = model.last_linear\n","        del model.last_linear\n","        state_dict = torch.load(\n","            '/home/ondyari/.torch/models/xception-b5690688.pth')\n","        for name, weights in state_dict.items():\n","            if 'pointwise' in name:\n","                state_dict[name] = weights.unsqueeze(-1).unsqueeze(-1)\n","        model.load_state_dict(state_dict)\n","        model.last_linear = model.fc\n","        del model.fc\n","    return model\n","\n","\n","class TransferModel(nn.Module):\n","    \"\"\"\n","    Simple transfer learning model that takes an imagenet pretrained model with\n","    a fc layer as base model and retrains a new fc layer for num_out_classes\n","    \"\"\"\n","    def __init__(self, modelchoice, num_out_classes=2, dropout=0.0):\n","        super(TransferModel, self).__init__()\n","        self.modelchoice = modelchoice\n","        if modelchoice == 'xception':\n","            self.model = return_pytorch04_xception()\n","            # Replace fc\n","            num_ftrs = self.model.last_linear.in_features\n","            if not dropout:\n","                self.model.last_linear = nn.Linear(num_ftrs, num_out_classes)\n","            else:\n","                print('Using dropout', dropout)\n","                self.model.last_linear = nn.Sequential(\n","                    nn.Dropout(p=dropout),\n","                    nn.Linear(num_ftrs, num_out_classes)\n","                )\n","        elif modelchoice == 'resnet50' or modelchoice == 'resnet18':\n","            if modelchoice == 'resnet50':\n","                self.model = torchvision.models.resnet50(pretrained=True)\n","            if modelchoice == 'resnet18':\n","                self.model = torchvision.models.resnet18(pretrained=True)\n","            # Replace fc\n","            num_ftrs = self.model.fc.in_features\n","            if not dropout:\n","                self.model.fc = nn.Linear(num_ftrs, num_out_classes)\n","            else:\n","                self.model.fc = nn.Sequential(\n","                    nn.Dropout(p=dropout),\n","                    nn.Linear(num_ftrs, num_out_classes)\n","                )\n","        else:\n","            raise Exception('Choose valid model, e.g. resnet50')\n","\n","    def set_trainable_up_to(self, boolean, layername=\"Conv2d_4a_3x3\"):\n","        \"\"\"\n","        Freezes all layers below a specific layer and sets the following layers\n","        to true if boolean else only the fully connected final layer\n","        :param boolean:\n","        :param layername: depends on network, for inception e.g. Conv2d_4a_3x3\n","        :return:\n","        \"\"\"\n","        # Stage-1: freeze all the layers\n","        if layername is None:\n","            for i, param in self.model.named_parameters():\n","                param.requires_grad = True\n","                return\n","        else:\n","            for i, param in self.model.named_parameters():\n","                param.requires_grad = False\n","        if boolean:\n","            # Make all layers following the layername layer trainable\n","            ct = []\n","            found = False\n","            for name, child in self.model.named_children():\n","                if layername in ct:\n","                    found = True\n","                    for params in child.parameters():\n","                        params.requires_grad = True\n","                ct.append(name)\n","            if not found:\n","                raise Exception('Layer not found, cant finetune!'.format(\n","                    layername))\n","        else:\n","            if self.modelchoice == 'xception':\n","                # Make fc trainable\n","                for param in self.model.last_linear.parameters():\n","                    param.requires_grad = True\n","\n","            else:\n","                # Make fc trainable\n","                for param in self.model.fc.parameters():\n","                    param.requires_grad = True\n","\n","    def forward(self, x):\n","        x = self.model(x)\n","        return x\n","\n","\n","def model_selection(modelname, num_out_classes,\n","                    dropout=None):\n","    \"\"\"\n","    :param modelname:\n","    :return: model, image size, pretraining<yes/no>, input_list\n","    \"\"\"\n","    if modelname == 'xception':\n","        return TransferModel(modelchoice='xception',\n","                             num_out_classes=num_out_classes), 299, \\\n","               True, ['image'], None\n","    elif modelname == 'resnet18':\n","        return TransferModel(modelchoice='resnet18', dropout=dropout,\n","                             num_out_classes=num_out_classes), \\\n","               224, True, ['image'], None\n","    else:\n","        raise NotImplementedError(modelname)\n","\n","\n","if __name__ == '__main__':\n","    model, image_size, *_ = model_selection('resnet18', num_out_classes=2)\n","    print(model)\n","    model = model.cuda()\n","    from torchsummary import summary\n","    input_s = (3, image_size, image_size)\n","    print(summary(model, input_s))"]},{"cell_type":"markdown","metadata":{},"source":["# Pretrained Models from FaceForensics++\n","Pretrained models were downloaded from the referenced website here:\n","http://kaldir.vc.in.tum.de/FaceForensics/models/faceforensics++_models.zip\n","\n","We can load the model files using torch like this on cpu:\n","```\n","model = torch.load(model_path, map_location=torch.device('cpu'))\n","```\n","or with gpu/cuda:\n","```\n","model = torch.load(model_path)\n","model = model.cuda()\n","```\n","\n","## The authors provide different types of models:\n","From Appendix 4 of the [paper](https://arxiv.org/pdf/1901.08971.pdf) we see the published accuracy:\n","![](https://i.imgur.com/ebVvY9A.png)\n","\n","We have:\n","- Full image models\n","- Face_detected models\n","- RAW, compressed 23, and compressed 40"]},{"cell_type":"markdown","metadata":{},"source":["## Create a prediction function.\n","- This function is provided a video file, and runs the `test_full_image_network`\n","- Takes in the model file.\n","- Save the output avi file\n","- Display some of the frames of the output video"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metadata = pd.read_json('../input/deepfake-detection-challenge/train_sample_videos/metadata.json').T\n","\n","def predict_model(video_fn, model,\n","                  start_frame=0, end_frame=30,\n","                  plot_every_x_frames = 5):\n","    \"\"\"\n","    Given a video and model, starting frame and end frame.\n","    Predict on all frames.\n","    \n","    \"\"\"\n","    fn = video_fn.split('.')[0]\n","    label = metadata.loc[video_fn]['label']\n","    original = metadata.loc[video_fn]['original']\n","    video_path = f'../input/deepfake-detection-challenge/train_sample_videos/{video_fn}'\n","    output_path = './'\n","    test_full_image_network(video_path, model, output_path, start_frame=0, end_frame=30, cuda=False)\n","    # Read output\n","    vidcap = cv2.VideoCapture(f'{fn}.avi')\n","    success,image = vidcap.read()\n","    count = 0\n","    fig, axes = plt.subplots(3, 2, figsize=(20, 15))\n","    axes = axes = axes.flatten()\n","    i = 0\n","    while success:\n","        # Show every xth frame\n","        if count % plot_every_x_frames == 0:\n","\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            axes[i].imshow(image)\n","            axes[i].set_title(f'{fn} - frame {count} - true label: {label}')\n","            axes[i].xaxis.set_visible(False)\n","            axes[i].yaxis.set_visible(False)\n","            i += 1\n","        success,image = vidcap.read()\n","        count += 1\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Testing Full Image Models\n","## full_raw.p (full resolution)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["model_path = '../input/deepfakemodelspackages/faceforensics_models/faceforensics++_models_subset/full/xception/full_raw.p'\n","model = torch.load(model_path, map_location=torch.device('cpu'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predict_model('bbhtdfuqxq.mp4', model)\n","predict_model('crezycjqyk.mp4', model)\n","predict_model('ebchwmwayp.mp4', model)"]},{"cell_type":"markdown","metadata":{},"source":["## full_c40.p model"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["model_path_full_c40 = '../input/deepfakemodelspackages/faceforensics_models/faceforensics++_models_subset/full/xception/full_c40.p'\n","model_full_c40 = torch.load(model_path_full_c40, map_location=torch.device('cpu'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predict_model('bbhtdfuqxq.mp4', model_full_c40)\n","predict_model('crezycjqyk.mp4', model_full_c40)\n","predict_model('ebchwmwayp.mp4', model_full_c40)"]},{"cell_type":"markdown","metadata":{},"source":["## full_c23.p model"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["model_path_full23 = '../input/deepfakemodelspackages/faceforensics_models/faceforensics++_models_subset/full/xception/full_c23.p'\n","model_full_c23 = torch.load(model_path_full23, map_location=torch.device('cpu'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predict_model('bbhtdfuqxq.mp4', model_full_c23)\n","predict_model('crezycjqyk.mp4', model_full_c23)\n","predict_model('ebchwmwayp.mp4', model_full_c23)"]},{"cell_type":"markdown","metadata":{},"source":["# Testing face_detection/xcepion models\n","These models are specific to detecting just the face.\n","## face_detection/xcepion all_raw.p model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["model_path_face_allraw = '../input/deepfakemodelspackages/faceforensics_models/faceforensics++_models_subset/face_detection/xception/all_raw.p'\n","model_face_allraw = torch.load(model_path_face_allraw, map_location=torch.device('cpu'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predict_model('bbhtdfuqxq.mp4', model_face_allraw)\n","predict_model('crezycjqyk.mp4', model_face_allraw)\n","predict_model('ebchwmwayp.mp4', model_face_allraw)"]},{"cell_type":"markdown","metadata":{},"source":["##  face_detection/xcepion all_c40.p model"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["model_path_face_all_c40 = '../input/deepfakemodelspackages/faceforensics_models/faceforensics++_models_subset/face_detection/xception/all_c40.p'\n","model_face_all_c40 = torch.load(model_path_face_all_c40, map_location=torch.device('cpu'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predict_model('bbhtdfuqxq.mp4', model_face_all_c40)\n","predict_model('crezycjqyk.mp4', model_face_all_c40)\n","predict_model('ebchwmwayp.mp4', model_face_all_c40)"]},{"cell_type":"markdown","metadata":{},"source":["##  face_detection/xcepion all_c23.p model"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["model, *_ = model_selection(modelname='xception', num_out_classes=2)\n","model_path_face_allc23 = '../input/deepfakemodelspackages/faceforensics_models/faceforensics++_models_subset/face_detection/xception/all_c23.p'\n","model_face_all_c23 = torch.load(model_path_face_allc23, map_location=torch.device('cpu'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predict_model('bbhtdfuqxq.mp4', model_face_all_c23)\n","predict_model('crezycjqyk.mp4', model_face_all_c23)\n","predict_model('ebchwmwayp.mp4', model_face_all_c23)"]},{"cell_type":"markdown","metadata":{},"source":["# Validate Predictions on train set\n","- Predict for 50 frames\n","- Average the prediction of these frames to make a single prediction\n","- Test for some FAKE and REAL\n","- Return the maximum, minimum and average \"fake\" prediction of all the fames sampled\n","- If unable to detect a face, predict 0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def video_file_frame_pred(video_path, model,\n","                          start_frame=0, end_frame=300,\n","                          cuda=True, n_frames=5):\n","    \"\"\"\n","    Predict and give result as numpy array\n","    \"\"\"\n","    pred_frames = [int(round(x)) for x in np.linspace(start_frame, end_frame, n_frames)]\n","    predictions = []\n","    outputs = []\n","    # print('Starting: {}'.format(video_path))\n","\n","    # Read and write\n","    reader = cv2.VideoCapture(video_path)\n","\n","    video_fn = video_path.split('/')[-1].split('.')[0]+'.avi'\n","    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n","    fps = reader.get(cv2.CAP_PROP_FPS)\n","    num_frames = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n","    writer = None\n","\n","    # Face detector\n","    face_detector = dlib.get_frontal_face_detector()\n","\n","    # Text variables\n","    font_face = cv2.FONT_HERSHEY_SIMPLEX\n","    thickness = 2\n","    font_scale = 1\n","\n","    # Frame numbers and length of output video\n","    frame_num = 0\n","    assert start_frame < num_frames - 1\n","    end_frame = end_frame if end_frame else num_frames\n","    while reader.isOpened():\n","        _, image = reader.read()\n","        if image is None:\n","            break\n","        frame_num += 1\n","        if frame_num in pred_frames:\n","            height, width = image.shape[:2]\n","            # 2. Detect with dlib\n","            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","            faces = face_detector(gray, 1)\n","            if len(faces):\n","                # For now only take biggest face\n","                face = faces[0]\n","                # --- Prediction ---------------------------------------------------\n","                # Face crop with dlib and bounding box scale enlargement\n","                x, y, size = get_boundingbox(face, width, height)\n","                cropped_face = image[y:y+size, x:x+size]\n","\n","                # Actual prediction using our model\n","                prediction, output = predict_with_model(cropped_face, model,\n","                                                        cuda=cuda)\n","                predictions.append(prediction)\n","                outputs.append(output)\n","                # ------------------------------------------------------------------\n","        if frame_num >= end_frame:\n","            break\n","    # Figure out how to do this with torch\n","    preds_np = [x.detach().cpu().numpy()[0][1] for x in outputs]\n","    if len(preds_np) == 0:\n","        return predictions, outputs, 0.5, 0.5, 0.5\n","    try:\n","        mean_pred = np.mean(preds_np)\n","    except:\n","        # couldnt find faces\n","        mean_pred = 0.5\n","    min_pred = np.min(preds_np)\n","    max_pred = np.max(preds_np)\n","    return predictions, outputs, mean_pred, min_pred, max_pred"]},{"cell_type":"markdown","metadata":{},"source":["Because the face_detection/xception/all_c23.p model seemed to be returning the best restuls in our testing, we will use it for running"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["torch.nn.Module.dump_patches = True\n","model_path_23 = '../input/deepfakemodelspackages/faceforensics_models/faceforensics++_models_subset/face_detection/xception/all_c23.p'\n","model_23 = torch.load(model_path_23, map_location=torch.device('cpu'))\n","model_path_raw = '../input/deepfakemodelspackages/faceforensics_models/faceforensics++_models_subset/face_detection/xception/all_raw.p'\n","model_raw = torch.load(model_path_raw, map_location=torch.device('cpu'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Read metadata\n","metadata = pd.read_json('../input/deepfake-detection-challenge/train_sample_videos/metadata.json').T\n","\n","# Predict Fake\n","for video_fn in tqdm(metadata.query('label == \"FAKE\"').sample(77).index):\n","    video_path = f'../input/deepfake-detection-challenge/train_sample_videos/{video_fn}'\n","    predictions, outputs, mean_pred, min_pred, max_pred = video_file_frame_pred(video_path, model_23, n_frames=4, cuda=False)\n","    metadata.loc[video_fn, 'avg_pred_c23'] = mean_pred\n","    metadata.loc[video_fn, 'min_pred_c23'] = min_pred\n","    metadata.loc[video_fn, 'max_pred_c23'] = max_pred\n","    predictions, outputs, mean_pred, min_pred, max_pred = video_file_frame_pred(video_path, model_raw, n_frames=4, cuda=False)\n","    metadata.loc[video_fn, 'avg_pred_raw'] = mean_pred\n","    metadata.loc[video_fn, 'min_pred_raw'] = min_pred\n","    metadata.loc[video_fn, 'max_pred_raw'] = max_pred\n","    \n","# Predict Real\n","for video_fn in tqdm(metadata.query('label == \"REAL\"').sample(77).index):\n","    video_path = f'../input/deepfake-detection-challenge/train_sample_videos/{video_fn}'\n","    predictions, outputs, mean_pred, min_pred, max_pred = video_file_frame_pred(video_path, model_23, n_frames=4, cuda=False)\n","    metadata.loc[video_fn, 'avg_pred_c23'] = mean_pred\n","    metadata.loc[video_fn, 'min_pred_c23'] = min_pred\n","    metadata.loc[video_fn, 'max_pred_c23'] = max_pred\n","    predictions, outputs, mean_pred, min_pred, max_pred = video_file_frame_pred(video_path, model_raw, n_frames=4, cuda=False)\n","    metadata.loc[video_fn, 'avg_pred_raw'] = mean_pred\n","    metadata.loc[video_fn, 'min_pred_raw'] = min_pred\n","    metadata.loc[video_fn, 'max_pred_raw'] = max_pred"]},{"cell_type":"markdown","metadata":{},"source":["## Compute estimated score for 154 training samples"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["preds_df = metadata.dropna(subset=['avg_pred_raw']).copy()\n","preds_df['label_binary'] = 0\n","preds_df.loc[preds_df['label'] == \"FAKE\", 'label_binary'] = 1\n","preds_df[['min_pred_c23','max_pred_c23',\n","          'min_pred_raw','max_pred_raw']] = preds_df[['min_pred_c23','max_pred_c23',\n","                                                      'min_pred_raw','max_pred_raw']].fillna(0.5)\n","preds_df['naive_pred'] = 0.5\n","score_avg23 = log_loss(preds_df['label_binary'], preds_df['avg_pred_c23'])\n","score_min23 = log_loss(preds_df['label_binary'], preds_df['min_pred_c23'])\n","score_max23 = log_loss(preds_df['label_binary'], preds_df['max_pred_c23'])\n","score_avgraw = log_loss(preds_df['label_binary'], preds_df['avg_pred_raw'])\n","score_minraw = log_loss(preds_df['label_binary'], preds_df['min_pred_raw'])\n","score_maxraw = log_loss(preds_df['label_binary'], preds_df['max_pred_raw'])\n","score_naive = log_loss(preds_df['label_binary'], preds_df['naive_pred'])\n","preds_df['max_pred_clipped'] = preds_df['max_pred_c23'].clip(0.4, 1)\n","score_max_clipped = log_loss(preds_df['label_binary'], preds_df['max_pred_clipped'])\n","preds_df['max_pred_clipped_raw'] = preds_df['max_pred_raw'].clip(0.4, 1)\n","score_max_clipped_raw = log_loss(preds_df['label_binary'], preds_df['max_pred_clipped_raw'])\n","print('Score using average prediction of all frames all_c23.p: {:0.4f}'.format(score_avg23))\n","print('Score using minimum prediction of all frames all_c23.p: {:0.4f}'.format(score_min23))\n","print('Score using maximum prediction of all frames all_c23.p: {:0.4f}'.format(score_max23))\n","print('Score using 0.5 prediction of all frames: {:0.4f}'.format(score_naive))\n","print('Score using maximum clipped prediction of all frames: {:0.4f}'.format(score_max_clipped))\n","print('Score using average prediction of all frames all_raw.p: {:0.4f}'.format(score_avgraw))\n","print('Score using minimum prediction of all frames all_raw.p: {:0.4f}'.format(score_minraw))\n","print('Score using maximum prediction of all frames all_raw.p: {:0.4f}'.format(score_maxraw))\n","print('Score using maximum clipped prediction of all frames all_raw.p: {:0.4f}'.format(score_max_clipped_raw))"]},{"cell_type":"markdown","metadata":{},"source":["# Plot the Average vs Max Prediction Probability - Fake vs Real\n","The model appears to preform fairly poorly but shows that it is picking up on some signal."]},{"cell_type":"markdown","metadata":{},"source":["## c23 model results"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(1,1, figsize=(10, 10))\n","sns.scatterplot(x='avg_pred_c23', y='max_pred_c23', data=metadata.dropna(subset=['avg_pred_c23']), hue='label')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## raw model results"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(1,1, figsize=(10, 10))\n","sns.scatterplot(x='avg_pred_raw', y='max_pred_raw', data=metadata.dropna(subset=['avg_pred_raw']), hue='label')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i, d in metadata.groupby('label'):\n","    d['avg_pred_c23'].plot(kind='hist', figsize=(15, 5), bins=20, alpha=0.8, title='Average Prediction distribution c23')\n","    plt.legend(['FAKE','REAL'])\n","plt.show()\n","for i, d in metadata.groupby('label'):\n","    d['max_pred_c23'].plot(kind='hist', figsize=(15, 5), bins=20, title='Max Prediction distribution c23', alpha=0.8)\n","    plt.legend(['FAKE','REAL'])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i, d in metadata.groupby('label'):\n","    d['avg_pred_raw'].plot(kind='hist',\n","                           figsize=(15, 5),\n","                           bins=20,\n","                           alpha=0.8,\n","                           title='Average Prediction distribution raw')\n","    plt.legend(['FAKE','REAL'])\n","plt.show()\n","for i, d in metadata.groupby('label'):\n","    d['max_pred_raw'].plot(kind='hist',\n","                           figsize=(15, 5),\n","                           bins=20,\n","                           title='Max Prediction distribution raw',\n","                           alpha=0.8)\n","    plt.legend(['FAKE','REAL'])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metadata['max_pred_c23'] = metadata['max_pred_c23'].round(6)\n","metadata.dropna(subset=['max_pred_c23']).sort_values('label')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["metadata['label_binary'] = 0\n","metadata.loc[metadata['label'] == \"FAKE\", 'label_binary'] = 1"]},{"cell_type":"markdown","metadata":{},"source":["# Predict on test set.\n","- Predict for a subset of frames per video to cut down on run time\n","- Save the min/max and average prediction of all frames\n","- Save results to sample submission CSV."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","ss = pd.read_csv('../input/deepfake-detection-challenge/sample_submission.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for video_fn in tqdm(ss['filename'].unique()):\n","    video_path = f'../input/deepfake-detection-challenge/test_videos/{video_fn}'\n","    predictions, outputs, mean_pred, min_pred, max_pred = video_file_frame_pred(video_path, model, n_frames=4, cuda=False)\n","    ss.loc[ss['filename'] == video_fn, 'avg_pred'] = mean_pred\n","    ss.loc[ss['filename'] == video_fn, 'min_pred'] = min_pred\n","    ss.loc[ss['filename'] == video_fn, 'max_pred'] = max_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Use the Maximum frame predicted as \"Fake\" to be the final prediction\n","ss['label'] = ss['max_pred'].fillna(0.5).clip(0.4, 0.8)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ss['label'].plot(kind='hist', figsize=(15, 5), bins=50)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ss[['filename','label']].to_csv('submission.csv', index=False)\n","ss.to_csv('submission_min_max.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ss.head(20)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}